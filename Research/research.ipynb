{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a08d85c",
   "metadata": {},
   "source": [
    "# **DATA RELATED STUFF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c39d1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.12)\n",
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/mohamedhanyyy/chest-ctscan-images?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119M/119M [00:19<00:00, 6.25MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting model files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\arpit\\.cache\\kagglehub\\datasets\\mohamedhanyyy\\chest-ctscan-images\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mohamedhanyyy/chest-ctscan-images\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c374db48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Projects\\\\Chest-Cancer-Classification-App\\\\1'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.move(path, \"C:\\Projects\\Chest-Cancer-Classification-App\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2716464",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown \n",
    "\n",
    "file_id = \"1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3\"\n",
    "url = f\"https://drive.google.com/uc?/export=download&id={file_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f761cf31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://drive.google.com/uc?/export=download&id=1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3\n"
     ]
    }
   ],
   "source": [
    "print(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea44080e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?/export=download&id=1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3\n",
      "From (redirected): https://drive.google.com/uc?%2Fexport=download&id=1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3&confirm=t&uuid=1e2b5a97-2cbe-4fa6-9d2c-33d48180ff0b\n",
      "To: c:\\Projects\\Chest-Cancer-Classification-App\\Research\\Chest-Data.zip\n",
      "100%|██████████| 124M/124M [00:18<00:00, 6.57MB/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Chest-Data.zip'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdown.download(url, \"Chest-Data.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef7c10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a502c7e7",
   "metadata": {},
   "source": [
    "# **DATA INGESTION STEP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "104e6cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\Chest-Cancer-Classification-App\\\\Research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19c40246",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99f5daf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\Chest-Cancer-Classification-App'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b687aa8",
   "metadata": {},
   "source": [
    "### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "648f7a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc409c9f",
   "metadata": {},
   "source": [
    "### CONFIG.YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f142c501",
   "metadata": {},
   "source": [
    "- artifacts_root: artifacts\n",
    "\n",
    "- data_ingestion:\n",
    "  - root_dir: artifacts/data_ingestion\n",
    "  - source_url: https://drive.google.com/uc?/export=download&id=1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3\n",
    "  - data_dir: artifacts/data_ingestion/data.zip\n",
    "  - unzip_dir: artifacts/data_ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b141eaf4",
   "metadata": {},
   "source": [
    "### CONFIG ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bef85d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_url: str\n",
    "    data_dir: Path\n",
    "    unzip_dir: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742120b",
   "metadata": {},
   "source": [
    "### CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50a88b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "from src.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.utils import create_directories, read_yaml\n",
    "## from src.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "class AppConfig:\n",
    "    def __init__(self):\n",
    "        self.config_filepath = CONFIG_FILE_PATH\n",
    "        self.params_filepath = PARAMS_FILE_PATH\n",
    "\n",
    "        self.config = read_yaml(self.config_filepath)\n",
    "        self.params = read_yaml(self.params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])   ### Now always pass path to this function as list\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])              ### Now always pass path to this function as list\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            source_url = config.source_url,\n",
    "            data_dir = config.data_dir,\n",
    "            unzip_dir = config.unzip_dir\n",
    "        )\n",
    "        \n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e191639",
   "metadata": {},
   "source": [
    "### COMPONENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97771f06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\anaconda3\\envs\\tensor\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "import gdown\n",
    "import os, sys\n",
    "import zipfile\n",
    "## from src.configuration.configuration import AppConfig\n",
    "\n",
    "class DataIngestion:\n",
    "    def __init__(self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def download_data(self) -> str:\n",
    "        try:\n",
    "            root_dir = self.config.root_dir\n",
    "            data_url = self.config.source_url\n",
    "            zip_download_dir = self.config.data_dir\n",
    "\n",
    "            create_directories([root_dir])            ### Now always pass path to this function as list\n",
    "\n",
    "            gdown.download(data_url, zip_download_dir)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)\n",
    "    \n",
    "    def unzip_data(self):\n",
    "        try:\n",
    "            unzip_dir = self.config.unzip_dir\n",
    "            zip_download_dir = self.config.data_dir\n",
    "\n",
    "            create_directories([unzip_dir])             ### Now always pass path to this function as list\n",
    "\n",
    "            with zipfile.ZipFile(zip_download_dir, 'r') as zip_ref:\n",
    "                zip_ref.extractall(unzip_dir)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69d8034",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fd4f9fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-06-25 04:01:12]   19 | INFO     | yaml file: config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m[2025-06-25 04:01:12]   19 | INFO     | yaml file: params.yaml loaded successfully\u001b[0m\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?/export=download&id=1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3\n",
      "From (redirected): https://drive.google.com/uc?%2Fexport=download&id=1Rfn_h7aGCgpSAuZJVm2oGM31Rl6f2Xs3&confirm=t&uuid=5c07c6eb-c2b1-4613-b485-e79f90b567a0\n",
      "To: c:\\Projects\\Chest-Cancer-Classification-App\\artifacts\\data_ingestion\\data.zip\n",
      "100%|██████████| 124M/124M [00:18<00:00, 6.58MB/s] \n"
     ]
    }
   ],
   "source": [
    "## from src.configuration.configuration import AppConfig\n",
    "## from src.components.data_ingestion import DataIngestion\n",
    "import sys\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "\n",
    "try:\n",
    "    config = AppConfig()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion_obj = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion_obj.download_data()\n",
    "    data_ingestion_obj.unzip_data()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1860c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b445fad0",
   "metadata": {},
   "source": [
    "# **BASE MODEL STEP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf24e2",
   "metadata": {},
   "source": [
    "### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be3b83a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7c1487",
   "metadata": {},
   "source": [
    "### CONFIG.YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1352ea39",
   "metadata": {},
   "source": [
    "- base_model_preparation:\n",
    "  - root_dir: artifacts/base_model\n",
    "  - base_model_path: artifacts/base_model/base_model.h5\n",
    "  - updated_base_model_path: artifacts/base_modeln/updated_base_model.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc94fbb2",
   "metadata": {},
   "source": [
    "### PARAMS.YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90724406",
   "metadata": {},
   "source": [
    "- AUGMENTATION: True\n",
    "- IMAGE_SIZE: [224, 224, 3]   ## as per VGG19\n",
    "- BATCH_SIZE: 32\n",
    "- INCLUDE_TOP: False\n",
    "- EPOCHS: 1\n",
    "- CLASSES: 4\n",
    "- WEIGHTS: imagenet\n",
    "- LEARNING_RATE: 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76b02a",
   "metadata": {},
   "source": [
    "### CONFIG ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4f07ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class PrepareBaseModelConfig:\n",
    "    root_dir: Path\n",
    "    base_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    augmentation: bool\n",
    "    image_size: list\n",
    "    learning_rate: float\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    num_classes: int\n",
    "    include_top: bool\n",
    "    weights: str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20b104b",
   "metadata": {},
   "source": [
    "### CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57951f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "from src.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.utils import create_directories, read_yaml\n",
    "##from src.entity.config_entity import DataIngestionConfig\n",
    "\n",
    "class AppConfig:\n",
    "    def __init__(self):\n",
    "        self.config_filepath = CONFIG_FILE_PATH\n",
    "        self.params_filepath = PARAMS_FILE_PATH\n",
    "\n",
    "        self.config = read_yaml(self.config_filepath)\n",
    "        self.params = read_yaml(self.params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])   ### Now always pass path to this function as list\n",
    "    \n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])              ### Now always pass path to this function as list\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir = config.root_dir,\n",
    "            source_url = config.source_url,\n",
    "            data_dir = config.data_dir,\n",
    "            unzip_dir = config.unzip_dir\n",
    "        )\n",
    "        \n",
    "        return data_ingestion_config\n",
    "\n",
    "    def get_base_model_config(self) -> PrepareBaseModelConfig:\n",
    "        config = self.config.base_model_preparation\n",
    "        params = self.params\n",
    "\n",
    "        create_directories([config.root_dir])            ### Now always pass path to this function as list\n",
    "\n",
    "        base_model_config = PrepareBaseModelConfig(\n",
    "            root_dir = Path(config.root_dir),\n",
    "            base_model_path = Path(config.base_model_path),\n",
    "            updated_base_model_path = Path(config.updated_base_model_path),\n",
    "            augmentation = params.AUGMENTATION,\n",
    "            image_size = params.IMAGE_SIZE,\n",
    "            learning_rate = params.LEARNING_RATE,\n",
    "            epochs = params.EPOCHS,\n",
    "            batch_size = params.BATCH_SIZE,\n",
    "            num_classes = params.CLASSES,\n",
    "            include_top = params.INCLUDE_TOP,\n",
    "            weights = params.WEIGHTS\n",
    "        )\n",
    "\n",
    "        return base_model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4e05be",
   "metadata": {},
   "source": [
    "### COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ce30c0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "import urllib.request as request\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "## from src.configuration.configuration import AppConfig\n",
    "## from src.entity.config_entity import PrepareBaseModelConfig\n",
    "\n",
    "class PrepareBaseModel:\n",
    "    def __init__(self, config: PrepareBaseModelConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n",
    "\n",
    "    def download_base_mode(self):\n",
    "        self.model = tf.keras.applications.VGG19(\n",
    "            input_shape=self.config.image_size,\n",
    "            include_top=self.config.include_top,\n",
    "            weights=self.config.weights,\n",
    "        )\n",
    "\n",
    "        self.save_model(path=self.config.base_model_path, model=self.model)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _prepare_full_model(model, classes, freeze_all, freeze_till, learning_rate):\n",
    "        if freeze_all:\n",
    "            for layer in model.layers:\n",
    "                model.trainable = False\n",
    "        elif (freeze_till is not None) and (freeze_till > 0):\n",
    "            for layer in model.layers[:-freeze_till]:\n",
    "                model.trainable = False\n",
    "            \n",
    "        flatten_in = tf.keras.layers.Flatten()(model.output)\n",
    "        prediction = tf.keras.layers.Dense(\n",
    "            units=classes,\n",
    "            activation=\"softmax\"\n",
    "        )(flatten_in)\n",
    "\n",
    "        full_model = tf.keras.models.Model(\n",
    "            inputs = model.input,\n",
    "            outputs = prediction\n",
    "        )\n",
    "\n",
    "        full_model.compile(\n",
    "            optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "            loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        full_model.summary()\n",
    "        \n",
    "        return full_model\n",
    "\n",
    "    def update_base_model(self):\n",
    "        self.full_model = self._prepare_full_model(\n",
    "            model = self.model,\n",
    "            classes = self.config.num_classes,\n",
    "            freeze_all=True,\n",
    "            freeze_till=None,\n",
    "            learning_rate=self.config.learning_rate\n",
    "        )\n",
    "\n",
    "        self.save_model(path=self.config.updated_base_model_path, model=self.full_model)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8e802c",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa9fd511",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-06-25 15:12:58]   19 | INFO     | yaml file: config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m[2025-06-25 15:12:58]   19 | INFO     | yaml file: params.yaml loaded successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv4 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv4 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv4 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 25088)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 100356    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,124,740\n",
      "Trainable params: 100,356\n",
      "Non-trainable params: 20,024,384\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = AppConfig()\n",
    "    prepare_base_model = config.get_base_model_config()\n",
    "    prepare_base_model =  PrepareBaseModel(config=prepare_base_model)\n",
    "    prepare_base_model.download_base_mode()\n",
    "    prepare_base_model.update_base_model()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c4cd3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e765a03e",
   "metadata": {},
   "source": [
    "# **MODEL TRAINER STEP**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac7efd",
   "metadata": {},
   "source": [
    "### CONSTANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aaea36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CONFIG_FILE_PATH = Path(\"config/config.yaml\")\n",
    "PARAMS_FILE_PATH = Path(\"params.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d76547a",
   "metadata": {},
   "source": [
    "### CONFIG.YAML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c36d88",
   "metadata": {},
   "source": [
    "- model_trainer:\n",
    "  - root_dir: artifacts/model_trainer\n",
    "  - trained_model_path: artifacts/model_trainer/trained_model.h5\n",
    "  - train_data_path: artifacts/data_ingestion/Data/train\n",
    "  - valid_data_path: artifacts/data_ingestion/Data/valid\n",
    "  - test_data_path: artifacts/data_ingestion/Data/test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5366a4e5",
   "metadata": {},
   "source": [
    "### CONFIG ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cc91824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    updated_base_model_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    valid_data_path: Path\n",
    "    augmentation: bool\n",
    "    image_size: list\n",
    "    epochs: int\n",
    "    batch_size: int\n",
    "    train_history_dir: Path\n",
    "    loss_images_path: Path\n",
    "    accuracy_images_path: Path\n",
    "    history_json_path: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d3dc0a",
   "metadata": {},
   "source": [
    "### CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1c41e1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "from src.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "from src.utils import create_directories, read_yaml\n",
    "## from src.entity.config_entity import ModelTrainerConfig\n",
    "from pathlib import Path\n",
    "\n",
    "class AppConfig:\n",
    "    def __init__(self):\n",
    "        self.config_filepath = CONFIG_FILE_PATH\n",
    "        self.params_filepath = PARAMS_FILE_PATH\n",
    "\n",
    "        self.config = read_yaml(self.config_filepath)\n",
    "        self.params = read_yaml(self.params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])   ### Now always pass path to this function as list\n",
    "    \n",
    "    def get_model_trainer_config(self):\n",
    "        model_trainer = self.config.model_trainer\n",
    "        params = self.params\n",
    "        base_model = self.config.base_model_preparation\n",
    "\n",
    "        create_directories([model_trainer.root_dir])              ### Now always pass path to this function as list\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir = Path(model_trainer.root_dir),\n",
    "            trained_model_path = Path(model_trainer.trained_model_path),\n",
    "            updated_base_model_path = Path(base_model.updated_base_model_path),\n",
    "            train_data_path= Path(model_trainer.train_data_path),\n",
    "            test_data_path= Path(model_trainer.test_data_path),\n",
    "            valid_data_path= Path(model_trainer.valid_data_path),\n",
    "            augmentation = params.AUGMENTATION,\n",
    "            image_size = params.IMAGE_SIZE,\n",
    "            epochs = params.EPOCHS,\n",
    "            batch_size = params.BATCH_SIZE,\n",
    "            train_history_dir = Path(model_trainer.train_history_dir),\n",
    "            loss_images_path = Path(model_trainer.loss_images_path),\n",
    "            accuracy_images_path = Path(model_trainer.accuracy_images_path),\n",
    "            history_json_path = Path(model_trainer.history_json_path)\n",
    "        )\n",
    "        \n",
    "        return model_trainer_config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a938e5",
   "metadata": {},
   "source": [
    "### COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "124252c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "import urllib.request as request\n",
    "import os, sys\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "## from src.configuration.configuration import AppConfig\n",
    "## from src.entity.config_entity import ModelTrainerConfig\n",
    "from src.utils import create_directories, save_json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def get_model(self):\n",
    "        self.model = tf.keras.models.load_model(\n",
    "            self.config.updated_base_model_path\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_img(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image, label\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_model(path: Path, model: tf.keras.Model):\n",
    "        model.save(path)\n",
    "\n",
    "    def get_data_for_training(self):\n",
    "\n",
    "        self.train_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory=self.config.train_data_path,\n",
    "            batch_size=self.config.batch_size,\n",
    "            interpolation = \"bilinear\",\n",
    "            image_size = self.config.image_size[:-1],\n",
    "            label_mode=\"categorical\",\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        self.valid_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory=self.config.valid_data_path,\n",
    "            batch_size=self.config.batch_size,\n",
    "            interpolation = \"bilinear\",\n",
    "            image_size = self.config.image_size[:-1],\n",
    "            label_mode=\"categorical\",\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        self.test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory=self.config.test_data_path,\n",
    "            image_size=self.config.image_size[:-1],\n",
    "            interpolation=\"bilinear\",\n",
    "            batch_size=self.config.batch_size,\n",
    "            label_mode=\"categorical\",\n",
    "            shuffle=False\n",
    "        )\n",
    "    \n",
    "    def train(self):\n",
    "\n",
    "        early_stopping = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        checkpoint = ModelCheckpoint(\n",
    "            filepath=str(self.config.trained_model_path),\n",
    "            monitor='val_loss',\n",
    "            save_best_only=True\n",
    "        )\n",
    "\n",
    "        self.history = self.model.fit(\n",
    "            self.train_data.map(ModelTrainer.normalize_img),\n",
    "            validation_data=self.valid_data.map(ModelTrainer.normalize_img),\n",
    "            epochs=self.config.epochs,\n",
    "            callbacks=[early_stopping, checkpoint],\n",
    "            )\n",
    "        \n",
    "        self.save_model(path=self.config.trained_model_path, model=self.model)\n",
    "    \n",
    "    def save_training_metrics(self):\n",
    "        try:\n",
    "            history_dir = Path(self.config.train_history_dir)\n",
    "            metrics_path = self.config.history_json_path\n",
    "            loss_image = self.config.loss_images_path\n",
    "            acc_image = self.config.accuracy_images_path\n",
    "\n",
    "            create_directories([history_dir])\n",
    "\n",
    "            history_dict = self.history.history\n",
    "\n",
    "            save_json(path=metrics_path, data=history_dict)\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.plot(history_dict['loss'], label='Train Loss')\n",
    "            plt.plot(history_dict['val_loss'], label='Val Loss')\n",
    "            plt.title('Loss Over Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.legend() \n",
    "            plt.savefig(loss_image)\n",
    "            plt.close()\n",
    "            logger.info(f\"Loss plot saved to {loss_image}\")\n",
    "\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            plt.plot(history_dict['accuracy'], label='Train Accuracy')\n",
    "            plt.plot(history_dict['val_accuracy'], label='Val Accuracy')\n",
    "            plt.title('Accuracy Over Epochs')\n",
    "            plt.xlabel('Epochs')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.legend()\n",
    "            plt.savefig(acc_image)\n",
    "            plt.close()\n",
    "            logger.info(f\"Accuracy plot saved to {acc_image}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(e)\n",
    "            raise CustomException(e, sys)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98bdfb3",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d9e1e3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-06-25 18:23:41]   19 | INFO     | yaml file: config\\config.yaml loaded successfully\u001b[0m\n",
      "\u001b[32m[2025-06-25 18:23:41]   19 | INFO     | yaml file: params.yaml loaded successfully\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 613 files belonging to 4 classes.\n",
      "Found 72 files belonging to 4 classes.\n",
      "Found 315 files belonging to 4 classes.\n",
      "Epoch 1/3\n",
      "77/77 [==============================] - 7s 84ms/step - loss: 2.0173 - accuracy: 0.3279 - val_loss: 2.2627 - val_accuracy: 0.3194\n",
      "Epoch 2/3\n",
      "77/77 [==============================] - 6s 80ms/step - loss: 1.3249 - accuracy: 0.5253 - val_loss: 1.6366 - val_accuracy: 0.4722\n",
      "Epoch 3/3\n",
      "77/77 [==============================] - 6s 78ms/step - loss: 0.9472 - accuracy: 0.6150 - val_loss: 1.7028 - val_accuracy: 0.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2025-06-25 18:24:01]   43 | INFO     | json file saved at: artifacts\\model_trainer\\model_history\\model_history.json\u001b[0m\n",
      "\u001b[32m[2025-06-25 18:24:02]  104 | INFO     | Loss plot saved to artifacts\\model_trainer\\model_history\\loss_image.png\u001b[0m\n",
      "\u001b[32m[2025-06-25 18:24:02]  115 | INFO     | Accuracy plot saved to artifacts\\model_trainer\\model_history\\accuracy_image.png\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = AppConfig()\n",
    "    trainer_config = config.get_model_trainer_config()\n",
    "    training = ModelTrainer(config=trainer_config)\n",
    "    training.get_model()\n",
    "    training.get_data_for_training()\n",
    "    training.train()\n",
    "    training.save_training_metrics()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db92eb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f2225211",
   "metadata": {},
   "source": [
    "# **MODEL EVALUATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2848ca0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13329a39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Projects\\\\Chest-Cancer-Classification-App'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "060a3291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arpit\\anaconda3\\envs\\tensor\\lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4dcdf178",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(\n",
    "    \"artifacts/model_trainer/trained_model.h5\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a749d7fa",
   "metadata": {},
   "source": [
    "### CONFIG ENTITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ada991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class EvaluationConfig:\n",
    "    score_file: Path\n",
    "    model_path: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    all_params: dict\n",
    "    mlflow_uri: str\n",
    "    image_size: list\n",
    "    batch_size: int\n",
    "    loss_images_path: Path\n",
    "    accuracy_images_path: Path\n",
    "    history_json_path: Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c661a631",
   "metadata": {},
   "source": [
    "### CONFIGURATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20a8f781",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils import read_yaml, save_json, create_directories\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "from src.constants import CONFIG_FILE_PATH, PARAMS_FILE_PATH\n",
    "## from src.entity.config_entity import ModelTrainerConfig\n",
    "from pathlib import Path\n",
    "\n",
    "class AppConfig:\n",
    "    def __init__(self):\n",
    "        self.config_filepath = CONFIG_FILE_PATH\n",
    "        self.params_filepath = PARAMS_FILE_PATH\n",
    "\n",
    "        self.config = read_yaml(self.config_filepath)\n",
    "        self.params = read_yaml(self.params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])   ### Now always pass path to this function as list\n",
    "\n",
    "    def get_evaluation_config(self) -> EvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "        trainer_config = self.config.model_trainer\n",
    "\n",
    "        create_directories([config.root_dir])              ### Now always pass path to this function as list\n",
    "\n",
    "        evaluation_config = EvaluationConfig(\n",
    "            score_file=config.score_file, \n",
    "            model_path=config.trained_model_path,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            all_params=self.params,\n",
    "            mlflow_uri=os.environ.get(\"MLFLOW_TRACKING_URI\"),\n",
    "            image_size=self.params.IMAGE_SIZE,\n",
    "            batch_size=self.params.BATCH_SIZE,\n",
    "            loss_images_path=trainer_config.loss_images_path,\n",
    "            accuracy_images_path=trainer_config.accuracy_images_path,\n",
    "            history_json_path=trainer_config.history_json_path\n",
    "        )\n",
    "        \n",
    "        return evaluation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c778cb0",
   "metadata": {},
   "source": [
    "### COMPONENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa1538f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## from src.entity.config_entity import EvaluationConfig\n",
    "from src.logger import logger\n",
    "from src.exception import CustomException\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: EvaluationConfig):\n",
    "        self.config = config\n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_img(image, label):\n",
    "        image = tf.cast(image, tf.float32) / 255.0\n",
    "        return image, label\n",
    "\n",
    "    @staticmethod\n",
    "    def load_model(path: Path) -> tf.keras.Model:\n",
    "        return tf.keras.models.load_model(path)\n",
    "    \n",
    "    def get_test_data(self):\n",
    "        \n",
    "        self.test_data = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "            directory=self.config.test_data_path,\n",
    "            image_size=self.config.image_size[:-1],\n",
    "            interpolation=\"bilinear\",\n",
    "            batch_size=self.config.batch_size,\n",
    "            label_mode=\"categorical\",\n",
    "            shuffle=False\n",
    "        )\n",
    "\n",
    "        self.test_data = self.test_data.map(ModelEvaluation.normalize_img)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model = self.load_model(self.config.model_path)\n",
    "        self.results = self.model.evaluate(self.test_data)\n",
    "        self.score = {'loss': self.results[0], 'accuracy': self.results[1]}\n",
    "        save_json(path=Path(self.config.score_file), data=self.score)\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics(self.score)\n",
    "\n",
    "            if tracking_url_type_store != \"file\":\n",
    "                mlflow.keras.log_model(self.model, \"model\", registered_model_name=\"VGG16Model\")\n",
    "            else:\n",
    "                mlflow.keras.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf11538f",
   "metadata": {},
   "source": [
    "### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4da54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = AppConfig()\n",
    "    eval_config = config.get_evaluation_config()\n",
    "    evaluation = ModelEvaluation(eval_config)\n",
    "    evaluation.get_test_data()\n",
    "    evaluation.evaluate()\n",
    "    evaluation.log_into_mlflow()\n",
    "except Exception as e:\n",
    "    raise CustomException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491fc8a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
